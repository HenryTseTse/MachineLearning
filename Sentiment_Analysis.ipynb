{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Multinomial Naive Bayes algorithm for sentiment analysis\n",
    "\n",
    "In this assigment, you will learn to classify a movie review as 'positive' or 'negative' using Multinomial Naive Bayes.\n",
    "\n",
    "The dataset used in this assignment is a version of a dataset available at http://nifty.stanford.edu/2016/manley-urness-movie-review-sentiment/, which was previously used in a Stanford project and a Kaggle competition.  (See the page referenced in the URL for further details.)\n",
    "\n",
    "The data for this assignment is divided into four files: trainfilex.txt, trainfiley.txt, testfilex.txt, testfiley.txt\n",
    "\n",
    "Each line of trainfilex.txt contains a review of a film.  The labels of these reviews are in trainfiley.txt.\n",
    "Line i of trainfilex.txt has the label of the review in line i of trainfiley.txt.\n",
    "The test reviews are in testfilex.txt and their corresponding labels are in testfiley.txt.\n",
    "\n",
    "The reviews in these files have been pre-processed to replace punctuation with whitespace and to convert capital letters into lower case.\n",
    "\n",
    "You need to write a program that uses the Multinomial Naive Bayes algorithm to train on the training files\n",
    "and then predict the labels of the reviews in the test file.  Use smoothing with m=0.2 when estimating the P(w|C) quantities.  Use log likelihood to avoid underflow.  Don't try to be selective with your vocabulary.  Include all tokens from the training set in your vocabulary.\n",
    "\n",
    "Your program should calculate\n",
    "the prediction accuracy (percentage of correct predictions) achieved on the test reviews, by comparing\n",
    "the predictions made by your algorithm to the labels in testlabels.txt.  (Your program should NOT\n",
    "access testlabels.txt until it needs to calculate the prediction accuracy.)\n",
    "Do not use sklearn in your program.\n",
    "\n",
    "\n",
    "## Step 1:  Reading in the data files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' serious and thoughtful  ', ' with a completely predictable plot  you  ll swear that you  ve seen it all before  even if you  ve never come within a mile of the longest yard  ', ' if there was any doubt that peter o fallon did n t have an original bone in his body  a rumor of angels should dispel it  ', ' i like my christmas movies with more elves and snow and less pimps and ho  s  ', ' a terrifically entertaining specimen of spielbergian sci-fi  ']\n",
      "['1', '0', '0', '0', '1']\n",
      "The number of training examples:  1349\n",
      "The number of test examples:  151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read in train and test files, removing newlines\n",
    "\n",
    "f = open(\"trainfilex.txt\", \"r\")\n",
    "trainrevs = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "f = open(\"trainfiley.txt\",\"r\")\n",
    "trainlabels = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "f = open(\"testfilex.txt\",\"r\")\n",
    "testrevs = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "f = open(\"testfiley.txt\",\"r\")\n",
    "testlabels = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "# the first part of the training file contains all the\n",
    "# negative training reviews, and the second part contains all the positive\n",
    "# training reviews\n",
    "#\n",
    "# Check the first few lines of the trainrevs file\n",
    "print(trainrevs[0:5])\n",
    "# Check the first few lines of the trainlabels file\n",
    "print(trainlabels[0:5])\n",
    "\n",
    "print(\"The number of training examples: \", len(trainrevs))\n",
    "print(\"The number of test examples: \", len(testrevs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write two initial helper functions.\n",
    "###  Compute the vocabulary from the reviews in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a list of strings. each string consists of tokens separated by whitespace.\n",
    "# Output: a list of all distinct tokens found in the strings\n",
    "def build_vocab(x):\n",
    "########## TO DO ##########\n",
    "    \n",
    "    vocab = []\n",
    "    \n",
    "    for line in x:\n",
    "        for word in line.split():\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "            \n",
    "##########\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute smoothed estimate of P(w|C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: number n of occurences of w in C, total length N of docs in C, smoothing parameter m, \n",
    "# size of vocabulary vsize\n",
    "# Output: Smoothed estimate of P(w|C)\n",
    "def smooth_estimate(n,N,m,vsize):\n",
    "############# TO DO ###########\n",
    "    estimate = (n + m) / (N + (vsize * m))\n",
    "\n",
    "########\n",
    "    return estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the rest of your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training x and y\n",
    "\n",
    "#set m\n",
    "m = .2\n",
    "\n",
    "pos_count = 0\n",
    "for x in trainlabels:\n",
    "    if x == '1':\n",
    "        pos_count += 1\n",
    "num_pos = pos_count\n",
    "num_neg = len(trainlabels) - pos_count\n",
    "\n",
    "#priors of classes \n",
    "c_pos = num_pos / len(trainlabels)\n",
    "c_neg = num_neg / len(trainlabels)\n",
    "\n",
    "#building main vocab\n",
    "vocab_all = build_vocab(trainrevs)\n",
    "vocab_all_size = int(len(vocab_all))  \n",
    "\n",
    "#building pos/neg vocab\n",
    "pos_revs = []\n",
    "neg_revs = []\n",
    "for i in range(len(trainlabels)):\n",
    "    if trainlabels[i] == '1':\n",
    "        pos_revs.append(trainrevs[i])\n",
    "    else:\n",
    "        neg_revs.append(trainrevs[i])\n",
    "\n",
    "vocab_pos = build_vocab(pos_revs)\n",
    "vocab_neg = build_vocab(neg_revs)\n",
    "\n",
    "#build dic of word count for each class\n",
    "pos_dic = {}\n",
    "neg_dic = {}\n",
    "for i in range(len(trainlabels)):\n",
    "    if trainlabels[i] == '1':\n",
    "        for word in trainrevs[i].split():\n",
    "            if word in pos_dic.keys():\n",
    "                pos_dic[word] += 1\n",
    "            else:\n",
    "                pos_dic[word] = 1\n",
    "    else:   \n",
    "        for word in trainrevs[i].split():\n",
    "            if word in neg_dic.keys():\n",
    "                neg_dic[word] += 1\n",
    "            else:\n",
    "                neg_dic[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5456\n"
     ]
    }
   ],
   "source": [
    "print(vocab_all_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: P(y=0) 0.45959970348406226 \n",
      "\n",
      "b: P(y=1) 0.5404002965159377 \n",
      "\n",
      "c:\n",
      "P(intelligence|y=1) = 0.001148612829121753\n",
      "P(intelligence|y=0) = 0\n",
      "\n",
      "d:\n",
      "P(movie|y=1) =  0.01837780526594805\n",
      "P(movie|y=0) =  0.02614968440036069 \n",
      "\n",
      "e: accuracy (m = .2) =  0.8410596026490066\n",
      "Correct Count, Wrong_count:  127 24 \n",
      "\n",
      "f: accuracy (m = 1) = 0.8278145695364238\n",
      "Correct Count, Wrong_count:  125 26 \n",
      "\n",
      "g: \n",
      "positive count:  82\n",
      "negative count:  69\n",
      "zero_r accuracy:  0.543046357615894 \n",
      "\n",
      "h: sklearn accuracy: 0.847682119205298\n",
      "This is better than our multinomial naive bayes accuracy of 84.1%\n"
     ]
    }
   ],
   "source": [
    "#Questions:\n",
    "\n",
    "#a\n",
    "print('a: P(y=0)',c_neg,'\\n')\n",
    "\n",
    "#b\n",
    "print('b: P(y=1)',c_pos,'\\n')\n",
    "\n",
    "#c\n",
    "print('c:')\n",
    "print('P(intelligence|y=1) =',smooth_estimate(pos_dic['intelligence'],len(vocab_pos),m,vocab_all_size) )\n",
    "print('P(intelligence|y=0) = 0\\n') #doesn't exist\n",
    "\n",
    "print('d:')\n",
    "print('P(movie|y=1) = ',smooth_estimate(pos_dic['movie'],len(vocab_pos),m,vocab_all_size) )\n",
    "print('P(movie|y=0) = ',smooth_estimate(neg_dic['movie'],len(vocab_neg),m,vocab_all_size) ,'\\n')\n",
    "\n",
    "#e\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "\n",
    "for line in range(len(testrevs)):\n",
    "\n",
    "    prob_pos = 0\n",
    "    prob_neg = 0\n",
    "    \n",
    "    for word in testrevs[line].split():\n",
    "        \n",
    "        #positive\n",
    "        if word in pos_dic.keys(): #P(d|C)\n",
    "            prob_pos = prob_pos + np.log(smooth_estimate(pos_dic[word],len(vocab_pos),.2,vocab_all_size))\n",
    "        elif word in neg_dic.keys():\n",
    "            prob_pos = prob_pos + np.log(smooth_estimate(0,len(vocab_pos),.2,vocab_all_size))\n",
    "        #negative\n",
    "        if word in neg_dic.keys(): #P(d|C)\n",
    "            prob_neg = prob_neg + np.log(smooth_estimate(neg_dic[word],len(vocab_neg),.2,vocab_all_size))\n",
    "        elif word in pos_dic.keys():\n",
    "            prob_neg = prob_neg + np.log(smooth_estimate(0,len(vocab_neg),.2,vocab_all_size))\n",
    "    \n",
    "    prob_neg = prob_neg + np.log(c_neg) #cond * prior\n",
    "    prob_pos = prob_pos + np.log(c_pos) #cond * prior\n",
    "\n",
    "    if prob_pos >= prob_neg:\n",
    "        if testlabels[line] == '1':\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            wrong_count += 1\n",
    "    elif prob_pos < prob_neg:\n",
    "        if testlabels[line] == '0':\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            wrong_count += 1\n",
    "accuracy = correct_count / len(testrevs)\n",
    "print('e: accuracy (m = .2) = ',accuracy)\n",
    "print('Correct Count, Wrong_count: ',correct_count,wrong_count,'\\n')      \n",
    "\n",
    "\n",
    "#f\n",
    "correct_count = 0\n",
    "wrong_count = 0\n",
    "for line in range(len(testrevs)):\n",
    "\n",
    "    prob_pos = 0\n",
    "    prob_neg = 0\n",
    "    \n",
    "    for word in testrevs[line].split():\n",
    "        \n",
    "        #positive\n",
    "        if word in pos_dic.keys(): #P(d|C)\n",
    "            prob_pos = prob_pos + np.log(smooth_estimate(pos_dic[word],len(vocab_pos),1,vocab_all_size))\n",
    "        elif word in neg_dic.keys():\n",
    "            prob_pos = prob_pos + np.log(smooth_estimate(0,len(vocab_pos),1,vocab_all_size))\n",
    "        #negative\n",
    "        if word in neg_dic.keys(): #P(d|C)\n",
    "            prob_neg = prob_neg + np.log(smooth_estimate(neg_dic[word],len(vocab_neg),1,vocab_all_size))\n",
    "        elif word in pos_dic.keys():\n",
    "            prob_neg = prob_neg + np.log(smooth_estimate(0,len(vocab_neg),1,vocab_all_size))\n",
    "            \n",
    "    prob_neg = prob_neg + np.log(c_neg) #log cond + prior\n",
    "    prob_pos = prob_pos + np.log(c_pos) #log cond + prior\n",
    "\n",
    "    if prob_pos >= prob_neg:\n",
    "        if testlabels[line] == '1':\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            wrong_count += 1\n",
    "    elif prob_pos < prob_neg:\n",
    "        if testlabels[line] == '0':\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            wrong_count += 1\n",
    "accuracy1 = correct_count / len(testrevs)\n",
    "\n",
    "print('f: accuracy (m = 1) =',accuracy1)\n",
    "print('Correct Count, Wrong_count: ',correct_count,wrong_count,'\\n') \n",
    "\n",
    "\n",
    "#g\n",
    "pos = 0\n",
    "for i in range(len(testlabels)):\n",
    "    if testlabels[i] == '1':\n",
    "        pos += 1\n",
    "neg = len(testlabels) - pos\n",
    "if pos > neg:\n",
    "    zero_r = pos / len(testlabels)\n",
    "else:\n",
    "    zero_r = neg / len(testlabels)\n",
    "print('g: ')\n",
    "print('positive count: ', pos)\n",
    "print('negative count: ', neg)\n",
    "print('zero_r accuracy: ', zero_r,'\\n')\n",
    "\n",
    "\n",
    "#h\n",
    "print('h: sklearn accuracy: 0.847682119205298')\n",
    "print('This is better than our multinomial naive bayes accuracy of 84.1%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using sklearn on this dataset\n",
    "Sklearn has sophisticated tools that can be used to run Multinomial Naive Bayes\n",
    "on this dataset.  Let's explore those tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating the feature vector from the text (feature extraction)\n",
    "\n",
    "Each review will have its own feature vector.  The features will be the tokens in the vocabulary.\n",
    "The $j$th feature corresponds to the $j$th token in the vocabulary, and the value of $x_j$ for a review is the number of times\n",
    "that token appears in the review.  In each review, most of the features $x_j$ will be set to 0.\n",
    "\n",
    "We will use the sklearn method CountVectorize to create the feature vectors for every messge.\n",
    "This method creates the vocabulary and then creates the feature vectors for the reviews.\n",
    "In contrast to the approach we used above, of placing all tokens from the training set into the vocabulary, \n",
    "CountVectorize can be more selective.  \n",
    "\n",
    "CountVectorize can do the following (and more):\n",
    "* remove capitalization (already done for our files)\n",
    "* remove punctuation (already done for our files)\n",
    "* tokenize (i.e. split the document into individual words)\n",
    "* count frequencies of each token \n",
    "* remove 'stop words' (these are words that will not help us predict since they occur in most documents, e.g. 'a', 'and', 'the', 'him', 'is' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# creating an instance of CountVectorizer\n",
    "# Note there are issues with the way CountVectorizer removes stop words.  To learn more: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "#vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the 'stop words' \n",
    "#print(vectorizer.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847682119205298"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vocabulary for our feature transformation\n",
    "vectorizer.fit(trainrevs)\n",
    "\n",
    "# Next we create the feature vectors for the training data\n",
    "X_train = vectorizer.transform(trainrevs).toarray() # code to turn the training reviews into a feature vector\n",
    "X_test = vectorizer.transform(testrevs).toarray() # code to turn the test reviews into a feature vector\n",
    "\n",
    "# create the multinomial naive bayes classifier and fit it to the training data\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,trainlabels)\n",
    "\n",
    "# compute the accuracy of the classifier on the test set\n",
    "mnb.score(X_test,testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
